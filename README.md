# ITCS-6190 Assignment 3: AWS Data Processing Pipeline

**Name (ID):** Kiyoung Kim (801426261)  
**E-Mail:** kkim43@charlotte.edu  

This project demonstrates an end-to-end serverless data processing pipeline on AWS.  
I built an automated workflow that collects, processes, and visualizes order data without manual intervention.  
The process includes using Amazon S3 for storage, AWS Lambda for data processing, AWS Glue for cataloging, Athena for querying, and EC2 with Flask for dashboard visualization.

---

## 1. Amazon S3 Bucket Structure ü™£

**Approach:**  
I created an S3 bucket to organize the data lifecycle clearly into three stages ‚Äî raw input, processed output, and query results.

**Explanation:**  
The S3 bucket was named `itcs6190.assignment3.801426261.kkim43` and structured as follows:

```
‚îú‚îÄ‚îÄ raw/        # Original input data  
‚îú‚îÄ‚îÄ processed/  # Filtered output data from Lambda  
‚îî‚îÄ‚îÄ enriched/   # Athena query result storage  
```

This structure kept all files organized and enabled seamless integration between AWS Lambda, Glue, and Athena.

**Screenshot:**  
üì∏ *Figure 1.* Amazon S3 Bucket Structure  
![Screenshot1.png](./Screenshot/Screenshot1.png)

---

## 2. IAM Roles and Permissions üîê

**Approach:**  
To ensure secure communication between AWS services, I created three IAM roles with specific access policies.

**Explanation:**  
- **Lambda-S3-Processing-Role-Assignment3** ‚Äî allows Lambda to read and write S3 objects.  
- **Glue-S3-Crawler-Role-Assignment3** ‚Äî allows Glue to crawl and catalog processed S3 data.  
- **EC2-Athena-Dashboard-Role-Assignment3** ‚Äî allows EC2 to access Athena and S3 for dashboard visualization.

Each role was created using AWS‚Äôs managed policies to provide only the necessary permissions for each service.

**Screenshot:**  
üì∏ *Figure 2.* IAM Roles Created  
![Screenshot2.png](./Screenshot/Screenshot2.png)

---

## 3. Create the Lambda Function ‚öôÔ∏è

**Approach:**  
I developed a Lambda function that automatically filters raw data whenever new files are uploaded to S3.

**Explanation:**  
The function, named `FilterAndProcessOrders`, reads CSV files from the `raw/` folder, filters out invalid or outdated records, and saves the cleaned output to the `processed/` folder.  
This automation ensures all data entering the system is pre-validated and ready for analysis.

**Screenshot:**  
üì∏ *Figure 3.* Lambda Function Created  
![Screenshot3.png](./Screenshot/Screenshot3.png)

---

## 4. Configure the S3 Trigger ‚ö°

**Approach:**  
I connected the Lambda function to the S3 bucket using a trigger so it runs automatically when new data arrives.

**Explanation:**  
Whenever a new CSV file is uploaded to the `raw/` folder, S3 notifies Lambda, which then processes and writes the output to `processed/`.  
This setup eliminates manual execution and ensures consistent data processing.

**Screenshot:**  
üì∏ *Figure 4.* Configured S3 Trigger  
![Screenshot4.png](./Screenshot/Screenshot4.png)

**Processed Data Output:**  
After uploading the `Orders.csv` file, Lambda was triggered automatically.  
The resulting `filtered_orders.csv` file was created successfully in the `processed/` folder.

**Screenshot:**  
üì∏ *Figure 5.* Processed CSV File in the `processed/` Folder on S3  
![Screenshot5.png](./Screenshot/Screenshot5.png)

---

## 5. Create a Glue Crawler üï∏Ô∏è

**Approach:**  
To make the processed data queryable in Athena, I created an AWS Glue Crawler.

**Explanation:**  
The crawler scans files in the `processed/` folder, detects their schema, and automatically generates a table inside the `orders_db` database.  
This allows Athena to directly access the cleaned dataset for analysis.

**Screenshot:**  
üì∏ *Figure 6.* AWS Glue Crawler Configuration and Run Result  
![Screenshot6.png](./Screenshot/Screenshot6.png)

---

## 6. Query Data with Amazon Athena üîç

**Approach:**  
Once the data catalog was created by Glue, I queried it in Amazon Athena to extract insights and verify correctness.

**Explanation:**  
All queries were executed using the `orders_db` database and the `processed` table generated by the crawler.  
These queries summarized total sales, monthly performance, customer averages, and top orders.

**Queries executed:**
1. Total Sales by Customer  
2. Monthly Order Volume and Revenue  
3. Order Status Dashboard  
4. Average Order Value per Customer  
5. Top 10 Largest Orders in February 2025  

Each query ran successfully, and the results were exported to the `enriched/` folder in S3 for later use by the EC2 dashboard.

**Screenshot:**  
üì∏ *Figure 7.* Athena Query Results and Output Files  
![Screenshot7.png](./Screenshot/Screenshot7.png)

---

## 7. Launch the EC2 Web Server üñ•Ô∏è

**Approach:**  
I launched an EC2 instance to host a lightweight Flask web application that visualizes the Athena query results.

**Explanation:**  
The EC2 instance was created using Amazon Linux 2023 with a small free-tier instance type.  
It used the `EC2-Athena-Dashboard-Role-Assignment3` IAM role, allowing secure access to Athena and S3.  
After setting up SSH access, I installed Python, Flask, and Boto3 to enable server-side logic.

---

## 8. Connect to the EC2 Instance

**Approach:**  
After the EC2 instance was running, I connected via SSH using my downloaded key pair to configure the environment.

**Explanation:**  
The SSH connection provided direct terminal access for setting up and deploying the Flask web server.

```bash
ssh -i "assignment3-key.pem" ec2-user@<Your-Public-IP>
```

---

## 9. Set Up the Web Environment

**Approach:**  
I configured the EC2 environment to run the Flask-based web application connected to AWS services.

**Explanation:**  
The system was updated, and required dependencies (Python 3, Pip, Flask, and Boto3) were installed.  
This setup allowed smooth execution of the dashboard code without dependency issues.

---

## 10. Create and Configure the Web Application

**Approach:**  
I built a Flask web application that queries Athena and displays live data analytics from the processed dataset.

**Explanation:**  
I created a file named `app.py`, copied the contents of my script (`EC2InstanceNANOapp.py`), and updated the configuration variables:
- AWS region  
- Athena database (`orders_db`)  
- S3 path for Athena query results (`s3://itcs6190.assignment3.801426261.kkim43/enriched/`)  

Once configured, the app connected to Athena through Boto3 and generated a web dashboard showing all query outputs.

---

## 11. Run the App and View the Dashboard üöÄ

**Approach:**  
Finally, I executed the Flask application on the EC2 instance and accessed it through a browser.

**Explanation:**  
Running `python3 app.py` launched the server, and the dashboard became accessible via the EC2 public IP on port 5000.  
The web page displayed all Athena query results dynamically, formatted into organized tables.

**Screenshot:**  
üì∏ *Figure 8.* Final Web Dashboard ‚Äî Athena Orders Results on EC2  
![Screenshot8.png](./Screenshot/Screenshot8.png)

---

## Final Notes

- The pipeline successfully automates data ingestion, processing, and visualization using AWS‚Äôs core serverless tools.  
- Each stage ‚Äî from Lambda processing to EC2 visualization ‚Äî runs independently and securely.  
- To avoid extra costs, the EC2 instance and other active resources were stopped after testing was complete.
